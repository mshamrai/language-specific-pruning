# Language-Specific Pruning for Efficient Reduction of Large Language Models

This repository contains the code implementation for the paper "Language-Specific Pruning for Efficient Reduction of Large Language Models".


## Overview

This project focuses on exproring pruning techniques tailored to specific languages to enhance the efficiency of Large Language Models (LLMs). The key contribution lies in recognizing distinct language-specific weight distributions in LLMs trained on diverse languages. By leveraging this insight, the pruning process effectively compresses LLMs while preserving competitive performance. The code provided here allows for the reproduction of the results presented in the associated paper.


## Usage




## Acknowledgement

The main codebase was adapted from the [wanda](https://github.com/locuslab/wanda) repository.


## License

This project is licensed under the MIT License. Feel free to use and modify the code for academic and research purposes.

For inquiries, please contact m.shamrai at imath.kiev.ua 